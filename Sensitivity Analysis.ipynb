{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENSITIVITY CODE FOR AHP AND CONCORDANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "from scipy.stats import gmean\n",
    "from sympy import Point, Line, Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weightages = pd.read_excel(\"Weightages.xlsx\")\n",
    "Weightages_lst = list(Weightages[0])\n",
    "#print(Weightages_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the % of increment/decrement want to add for each value one at a time:  5\n"
     ]
    }
   ],
   "source": [
    "user_sensitivity_val = int(input(\"Enter the % of increment/decrement want to add for each value one at a time: \"))\n",
    "user_val = user_sensitivity_val/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('Sensitivity Weights for AHP and Concordance.xlsx', engine='xlsxwriter')\n",
    "j = 0\n",
    "\n",
    "for i in range(0,len(Weightages_lst)):\n",
    "    Weightages_1 = Weightages_lst[:]\n",
    "    sens_val_inc_dec = Weightages_1[i] + (Weightages_1[i] * user_val)\n",
    "    Weightages_1[i] = sens_val_inc_dec\n",
    "    globals()[\"sens_wts\" + str(i)] = Weightages_1\n",
    "    weights_df = globals()[\"sens_wts\" + str(i)]\n",
    "    weights_df = pd.DataFrame(weights_df)\n",
    "    \n",
    "    j += 1\n",
    "    weights_df.to_excel(writer, sheet_name = \"sens_wts\" + str(j))\n",
    "\n",
    "writer.save()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONCORDANCE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FieldData = pd.read_excel(\"Field Data Concordance.xlsx\")\n",
    "FieldData = FieldData.set_index(\"STRETCH NO.\")\n",
    "#print(FieldData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance = pd.ExcelWriter('Concordance rankings.xlsx', engine='xlsxwriter')\n",
    "j = 1\n",
    "for i in range(0,len(Weightages_lst)):\n",
    "    sens_wts = pd.read_excel(\"Sensitivity Weights for AHP and Concordance.xlsx\", sheet_name =\"sens_wts\" + str(j))\n",
    "    sens_wts = list(sens_wts.iloc[:,-1])\n",
    "    j += 1\n",
    "    \n",
    "    stretch_comp = []\n",
    "    for k in (range(len(FieldData))):\n",
    "        val1 = FieldData.iloc[k,:]\n",
    "        for l in range(len(FieldData)):\n",
    "            val2 = FieldData.iloc[l,:]\n",
    "\n",
    "            for l in range(len(val1)):\n",
    "                if val1[l] > val2[l]:\n",
    "                    comp_vals = sens_wts[l]\n",
    "                    stretch_comp.append(comp_vals)\n",
    "                else:\n",
    "                    #print(0)\n",
    "                    stretch_comp.append(0)\n",
    "\n",
    "    #print(stretch_comp)\n",
    "    n = len(stretch_comp)// len(FieldData.columns)\n",
    "    D1_array = np.array(stretch_comp)\n",
    "    D2_array = np.reshape(D1_array, (n, len(FieldData.columns)))\n",
    "    #print(D2_array)\n",
    "    \n",
    "    stretch_Df = pd.DataFrame(D2_array, columns = FieldData.columns)\n",
    "    \n",
    "    row_diff = ['m' + str(i) + \"-\" + \"m\" + str(j) for i in range(1, len(FieldData)+1) for j in range(1, len(FieldData)+1)]\n",
    "    stretch_Df[\"mi-mj\"] = row_diff\n",
    "    stretch_Df = stretch_Df.set_index(\"mi-mj\")\n",
    "    \n",
    "    sDF_rowWise_Sum = stretch_Df.loc[:].sum(axis = 1)\n",
    "    stretch_Df[\"SUM\"] = sDF_rowWise_Sum\n",
    "    #print(stretch_Df)\n",
    "    \n",
    "    SUM_vals = np.array(stretch_Df[\"SUM\"].tolist())\n",
    "    SUM_vals_2D = np.reshape(SUM_vals, (len(FieldData), len(FieldData)))\n",
    "    \n",
    "    Ranking = pd.DataFrame(SUM_vals_2D)\n",
    "    Ranking[\"STRETCH NO.\"] = FieldData.index\n",
    "    Ranking = Ranking.set_index(\"STRETCH NO.\")\n",
    "    \n",
    "    Ranking_rowWise_Sum = Ranking.loc[:].sum(axis = 1)\n",
    "    Ranking[\"SUM\"] = Ranking_rowWise_Sum\n",
    "    #print(Ranking)\n",
    "    \n",
    "    Concordance_Ranking = pd.DataFrame(list(Ranking[\"SUM\"]), columns = [\"SUM\"])\n",
    "    Concordance_Ranking[\"STRETCH NO.\"] = Ranking.index\n",
    "    Concordance_Ranking = Concordance_Ranking.set_index(\"STRETCH NO.\")\n",
    "    Concordance_Ranking[\"Rank\"] = Concordance_Ranking[\"SUM\"].rank(ascending = False, method = 'max')\n",
    "    Concordance_Ranking.to_excel(concordance, sheet_name = \"conc_rank\" + str(i+1) + \"_\" +str(user_sensitivity_val))\n",
    "    #print(str(i+1), Concordance_Ranking)\n",
    "concordance.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AHP CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_check_matrix_WSV = pd.read_excel(\"Consistency Check Matrix.xlsx\")\n",
    "C_check_matrix_WSV = C_check_matrix_WSV.set_index(\"CRITERIA\")\n",
    "Len_Matrix = len(C_check_matrix_WSV)\n",
    "#print(Len_Matrix)\n",
    "#print(C_check_matrix_WSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized_Data = pd.read_excel(\"Check Normalized Data.xlsx\")\n",
    "Normalized_Data = Normalized_Data.set_index(\"STRETCH NO.\")\n",
    "Normalized_Data_Matrix = np.array(Normalized_Data)\n",
    "#print(Normalized_Data_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHP = pd.ExcelWriter('AHP rankings.xlsx', engine='xlsxwriter')\n",
    "j = 1\n",
    "for i in range(0,len(Weightages_lst)):\n",
    "    sens_wts = pd.read_excel(\"Sensitivity Weights for AHP and Concordance.xlsx\", sheet_name =\"sens_wts\" + str(j))\n",
    "    sens_wts = list(sens_wts.iloc[:,-1])\n",
    "    j += 1\n",
    "    \n",
    "    C_check_matrix_WSV[\"Criteria Weights\"] = list(sens_wts)\n",
    "    #print(C_check_matrix_WSV)\n",
    "    \n",
    "    C_check_matrix_WSV['WSV / CW'] = C_check_matrix_WSV['WSV'] / C_check_matrix_WSV['Criteria Weights'] \n",
    "    Î»max = gmean(C_check_matrix_WSV[\"WSV / CW\"])\n",
    "    \n",
    "    Consistency_index = (Î»max - Len_Matrix) / (Len_Matrix - 1)\n",
    "    \n",
    "    n = Len_Matrix\n",
    "    if n > 0 and n < 51:\n",
    "        Random_Index_vals={1:0, 2:0, 3:0.58, 4:0.90, 5:1.12, 6:1.24, 7:1.32, 8:1.41, 9:1.45, 10:1.49, 11:1.51, 12:1.48, 13:1.56, 14:1.57, 15:1.59,\n",
    "                           16:1.6, 17:1.61, 18:1.61, 19:1.62, 20:1.63, 21:1.63, 22:1.64, 23:1.65, 24:1.65, 25:1.66, 26:1.66, 27:1.66, 28:1.67, 29:1.67, 30:1.67,\n",
    "                           31:1.67, 32:1.68, 33:1.68, 34:1.68, 35:1.68, 36:1.69, 37:1.69, 38:1.69, 39:1.69, 40: 1.69, 41:1.70, 42:1.70, 43:1.70, 44:1.70, 45:1.70,\n",
    "                           46:1.70, 47:1.70, 48:1.70, 49:1.71, 50:1.71}\n",
    "\n",
    "        Random_Index = Random_Index_vals.get(n)\n",
    "    else:\n",
    "        Random_Index = float(input(\"Enter the Random Index according to no. of criterias: \"))\n",
    "\n",
    "\n",
    "    Consistency_Ratio = Consistency_index / Random_Index\n",
    "\n",
    "    if Consistency_Ratio < 0.10:\n",
    "        Consistency_Ratio = np.dot(Normalized_Data_Matrix, sens_wts)\n",
    "        AHP_Ranking = pd.DataFrame()\n",
    "        AHP_Ranking[\"Priority Index\"] = Consistency_Ratio\n",
    "        AHP_Ranking[\"STRETCHES\"] = Normalized_Data.index\n",
    "        AHP_Ranking = AHP_Ranking.set_index(\"STRETCHES\")\n",
    "        AHP_Ranking[\"Rank\"] = AHP_Ranking[\"Priority Index\"].rank(ascending = False, method = 'max')\n",
    "        AHP_Ranking.to_excel(AHP, sheet_name = \"ahp_rank\" + str(i+1) + \"_\" +str(user_sensitivity_val))\n",
    "        #print(str(i+1), AHP_Ranking)\n",
    "    else:\n",
    "        print('The provided metrics are \"Inconsistent\".')\n",
    "        \n",
    "AHP.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENSITIVITY CODE FOR TOPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fuzzy_wts = pd.read_excel(\"Fuzzy wts.xlsx\")\n",
    "Fuzzy_wts = Fuzzy_wts.set_index(\"Criteria\")\n",
    "#print(len(Fuzzy_wts))\n",
    "Fuzzy_wts_list = list(Fuzzy_wts[2])\n",
    "#print(Fuzzy_wts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1 = pd.ExcelWriter('Sensitivity Weights for TOPSIS.xlsx', engine='xlsxwriter')\n",
    "j = 0\n",
    "\n",
    "for i in range(0,len(Fuzzy_wts)):\n",
    "    Fuzzy_wts_list_1 = Fuzzy_wts_list[:]\n",
    "    Fuzzy_wts_1 = Fuzzy_wts[:] \n",
    "    sens_val_inc_dec = Fuzzy_wts_list_1[i] + (Fuzzy_wts_list_1[i] * user_val)\n",
    "    Fuzzy_wts_list_1[i] = sens_val_inc_dec\n",
    "    globals()[\"sens_wts\" + str(i)] = Fuzzy_wts_list_1\n",
    "    weights_df = globals()[\"sens_wts\" + str(i)]\n",
    "    \n",
    "    Fuzzy_wts_1[2] = weights_df\n",
    "    weights_df_comp = pd.DataFrame(Fuzzy_wts_1)\n",
    "    #print(weights_df_comp)\n",
    "    \n",
    "    j += 1\n",
    "    weights_df_comp.to_excel(writer1, sheet_name = \"sens_wts_TOPSIS\" + str(j))\n",
    "\n",
    "writer1.save()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOPSIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized_data_topsis = pd.read_excel(\"Normalized Data Sheet.xlsx\")\n",
    "Normalized_data_topsis = Normalized_data_topsis.set_index(\"STRETCH NO.\")\n",
    "#print(Normalized_data_topsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating_Matrix = []\n",
    "for i in range(0, len(Normalized_data_topsis.columns)):\n",
    "    for j in range(0, len(Normalized_data_topsis)):\n",
    "        if Normalized_data_topsis.iloc[j,i] >= 0 and Normalized_data_topsis.iloc[j,i] <= 10:\n",
    "            Rating_Matrix.append(1)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 11 and Normalized_data_topsis.iloc[j,i] <= 20:\n",
    "            Rating_Matrix.append(2)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 21 and Normalized_data_topsis.iloc[j,i] <= 30:\n",
    "            Rating_Matrix.append(3)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 31 and Normalized_data_topsis.iloc[j,i] <= 40:\n",
    "            Rating_Matrix.append(4)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 41 and Normalized_data_topsis.iloc[j,i] <= 50:\n",
    "            Rating_Matrix.append(5)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 51 and Normalized_data_topsis.iloc[j,i] <= 60:\n",
    "            Rating_Matrix.append(6)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 61 and Normalized_data_topsis.iloc[j,i] <= 70:\n",
    "            Rating_Matrix.append(7)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 71 and Normalized_data_topsis.iloc[j,i] <= 80:\n",
    "            Rating_Matrix.append(8)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 81 and Normalized_data_topsis.iloc[j,i] <= 90:\n",
    "            Rating_Matrix.append(9)\n",
    "        elif Normalized_data_topsis.iloc[j,i] >= 91 and Normalized_data_topsis.iloc[j,i] <= 100:\n",
    "            Rating_Matrix.append(10)\n",
    "\n",
    "cols = len(Normalized_data_topsis)\n",
    "rows = len(Normalized_data_topsis.columns)\n",
    "Rating_Matrix = np.array(Rating_Matrix)\n",
    "Rating_Matrix = Rating_Matrix.reshape(rows,cols)\n",
    "Rating_Matrix = Rating_Matrix.transpose()\n",
    "#print(Rating_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topsis = pd.ExcelWriter('Fuzzy TOPSIS rankings.xlsx', engine='xlsxwriter')\n",
    "b = 1\n",
    "for a in range(0,len(Fuzzy_wts)):\n",
    "    sens_wts_topsis = pd.read_excel(\"Sensitivity Weights for TOPSIS.xlsx\", sheet_name =\"sens_wts_TOPSIS\" + str(b))\n",
    "    sens_wts_topsis = sens_wts_topsis.set_index(\"Criteria\")\n",
    "    #print(sens_wts_topsis)\n",
    "    b += 1\n",
    "\n",
    "    Rating_Matrix = np.array(Rating_Matrix)\n",
    "    Fuzzy_wts = np.array(sens_wts_topsis)\n",
    "    Fuzzy_Eval_vals = np.dot(Rating_Matrix, sens_wts_topsis)\n",
    "    Fuzzy_Eval_vals = pd.DataFrame(Fuzzy_Eval_vals)\n",
    "    Fuzzy_Eval_vals.columns = [\"l\",\"m\",\"n\"]\n",
    "    #print(Fuzzy_Eval_vals)\n",
    "\n",
    "\n",
    "    row_index = ['A' + str(i) for i in range(1, len(Fuzzy_Eval_vals)+1)]\n",
    "    Fuzzy_Eval_vals[\"Stretch No.\"] = row_index\n",
    "    Fuzzy_Eval_vals_1 = Fuzzy_Eval_vals.set_index(\"Stretch No.\")\n",
    "    #print(Fuzzy_Eval_vals_1)\n",
    "\n",
    "    \n",
    "    Triangular_FuzzyNums = pd.DataFrame(columns = ['l','m','n'])\n",
    "    for e in range(0,len(Fuzzy_Eval_vals)):\n",
    "        for f in range(0,len(Fuzzy_Eval_vals)):\n",
    "            arr1 = Fuzzy_Eval_vals.loc[e]\n",
    "            arr2 = Fuzzy_Eval_vals.loc[f]\n",
    "            diff = [arr1[0]-arr2[2],arr1[1]-arr2[1],arr1[2]-arr2[0]]\n",
    "            stretch_diff = pd.Series(diff, index = Triangular_FuzzyNums.columns)\n",
    "            Triangular_FuzzyNums = Triangular_FuzzyNums.append(stretch_diff, ignore_index=True)\n",
    "    row_diff = ['p' + str(e) + \"-\" + \"p\" + str(f) for e in range(1, len(Fuzzy_Eval_vals)+1) for f in range(1, len(Fuzzy_Eval_vals)+1)]\n",
    "    Triangular_FuzzyNums[\"p~i-p~j\"] = row_diff\n",
    "    Triangular_FuzzyNums_1 = Triangular_FuzzyNums.set_index(\"p~i-p~j\")\n",
    "    #print(Triangular_FuzzyNums_1)\n",
    "\n",
    "\n",
    "    Intersection_pnts = []\n",
    "    for k in range(0,len(Triangular_FuzzyNums)):\n",
    "        l = Triangular_FuzzyNums.iloc[k,0]\n",
    "        m = Triangular_FuzzyNums.iloc[k,1]\n",
    "        n = Triangular_FuzzyNums.iloc[k,2]\n",
    "    \n",
    "        if l < 0 and m > 0:\n",
    "            p1, p2, p3, p4 = Point(0, 0), Point(0, 1), Point(l, 0), Point(m, 1)\n",
    "            l1 = Line(p1, p2)\n",
    "            s1 = Segment(p3, p4)  \n",
    "            showIntersection = l1.intersection(s1)\n",
    "            #print(showIntersection)\n",
    "            Intersection_pnts.append(showIntersection)\n",
    "        elif m < 0 and n > 0: \n",
    "            p1, p2, p3, p4 = Point(0, 0), Point(0, 1), Point(n, 0), Point(m, 1)\n",
    "            l1 = Line(p1, p2)\n",
    "            s1 = Segment(p3, p4)  \n",
    "            showIntersection = l1.intersection(s1)\n",
    "            #print(showIntersection)\n",
    "            Intersection_pnts.append(showIntersection)\n",
    "        else:\n",
    "            Intersection_pnts.append([[0,0]])\n",
    "\n",
    "    \n",
    "    Intersection = []\n",
    "    for z in range(0,len(Triangular_FuzzyNums_1)):\n",
    "        Intersection.append(Intersection_pnts[z][0][1])\n",
    "\n",
    "    Triangular_FuzzyNums_1[\"Intersection height\"] = Intersection\n",
    "    #print(Triangular_FuzzyNums_1)\n",
    "\n",
    "   \n",
    "    E = []\n",
    "    for d in range(0,len(Triangular_FuzzyNums)):\n",
    "        l = Triangular_FuzzyNums_1.iloc[d,0]\n",
    "        m = Triangular_FuzzyNums_1.iloc[d,1]\n",
    "        n = Triangular_FuzzyNums_1.iloc[d,2]\n",
    "        h = round(Triangular_FuzzyNums_1.iloc[d,3], 3)\n",
    "\n",
    "        if l < 0 and m > 0 and n > 0:\n",
    "            pos_Area = (1/2 * (n-m)) + (1/2 * (m) * (1-h)) + (h * m)\n",
    "            neg_Area = 1/2 * (0-l) * h\n",
    "        elif m < 0 and n > 0:\n",
    "            pos_Area = 1/2 * (n) * h\n",
    "            neg_Area = (1/2 * abs((m-l))) + (1/2 * (0-m) * (1-h)) + ((0-m) * h)\n",
    "        elif n < 0:\n",
    "            pos_Area = 0\n",
    "            neg_Area = (1/2 * (n-m)) + (1/2 * (m-l))\n",
    "        else:\n",
    "            pos_Area = (1/2 * (n-m)) + (1/2 * (m-l))\n",
    "            neg_Area = 0\n",
    "\n",
    "        total_Area = pos_Area + neg_Area\n",
    "        corr_e = round(pos_Area/total_Area, 3)\n",
    "        E.append(corr_e)\n",
    "        \n",
    "    E_array = np.array(E)\n",
    "    TriFuzzNo_sqrt = int(np.sqrt(len(Triangular_FuzzyNums)))\n",
    "    E_2D_array = np.reshape(E_array, (TriFuzzNo_sqrt, TriFuzzNo_sqrt))\n",
    "    np.fill_diagonal(E_2D_array, 0.5)\n",
    "    #print(E_2D_array)\n",
    "\n",
    "    \n",
    "    PI = []\n",
    "    for x in range(0,len(E_2D_array)):\n",
    "        PI_Sum = sum(E_2D_array[x]) - (len(E_2D_array) * 0.5)\n",
    "        PI.append(PI_Sum)\n",
    "        \n",
    "    Ranking = pd.DataFrame()\n",
    "    Ranking[\"Priority Index\"] = PI\n",
    "    Ranking[\"Stretch No.\"] = row_index\n",
    "    Ranking[\"Rank\"] = Ranking[\"Priority Index\"].rank(ascending = False, method = 'max')\n",
    "    Ranking = Ranking.set_index(\"Stretch No.\")\n",
    "    \n",
    "    #print(str(a+1), Ranking)\n",
    "    Ranking.to_excel(Topsis, sheet_name = \"topsis_rank\" + str(a+1) + \"_\" +str(user_sensitivity_val))\n",
    "Topsis.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
